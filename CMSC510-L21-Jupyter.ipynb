{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEaXfXx2x01a"
   },
   "source": [
    "# Using PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic way to create a neural net in PyTorch is to specify all the matrices and vectors for weights/biases directly. Below, we see this approach used for a small neural net for classifying MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#read in the dataset, convert to numpy\n",
    "\n",
    "n_classes = 10;\n",
    "n_features = 28*28;\n",
    "\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "x_train = full_train_dataset.data.numpy().reshape(-1,n_features).astype(dtype=float)/255.0;\n",
    "x_test = full_test_dataset.data.numpy().reshape(-1,n_features).astype(dtype=float)/255.0;\n",
    "\n",
    "y_train_cat = full_train_dataset.targets.numpy()\n",
    "y_test_cat = full_test_dataset.targets.numpy()\n",
    "\n",
    "dd_y=pd.DataFrame(data=y_train_cat)\n",
    "y_train=pd.get_dummies(dd_y,columns = list(dd_y.columns)).to_numpy().astype(dtype=float);\n",
    "\n",
    "dd_y=pd.DataFrame(data=y_test_cat)\n",
    "y_test=pd.get_dummies(dd_y,columns = list(dd_y.columns)).to_numpy().astype(dtype=float);\n",
    "\n",
    "\n",
    "# permute the dataset\n",
    "permutation = np.random.permutation(x_train.shape[0]);\n",
    "x_train=x_train[permutation,:]\n",
    "y_train=y_train[permutation,:]\n",
    "\n",
    "# select smaller subset (out of 50000)\n",
    "subset_size = 50000;\n",
    "x_train=x_train[:subset_size,:]\n",
    "y_train=y_train[:subset_size,:]\n",
    "\n",
    "\n",
    "n_train=x_train.shape[0];\n",
    "\n",
    "# create tensor variables for data, we do not need gradient w.r.t. to them\n",
    "t_x_test=torch.tensor(x_test,requires_grad=False,device=device);\n",
    "t_y_test=torch.tensor(y_test,requires_grad=False,device=device);\n",
    "\n",
    "\n",
    "# number of activations (neurons) in the hidden layer\n",
    "n_hidden = 100;\n",
    "\n",
    "\n",
    "# create weights for layer 1 (W1, b1) and layer 2 (W2, b2)\n",
    "initialW1=0.01*np.random.randn(n_features,n_hidden)\n",
    "initialW2=0.01*np.random.randn(n_hidden,n_classes)\n",
    "\n",
    "W1 = torch.tensor(initialW1,requires_grad=True,device=device);\n",
    "b1 = torch.zeros((1,n_hidden),requires_grad=True,device=device);\n",
    "\n",
    "W2 = torch.tensor(initialW2,requires_grad=True,device=device);\n",
    "b2 = torch.zeros((1,n_classes),requires_grad=True,device=device);\n",
    "\n",
    "\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.0001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.SGD([W1,b1,W2,b2],lr=learning_rate)\n",
    "#optimizer = torch.optim.Adam([W1,b1,W2,b2],lr=learning_rate)\n",
    "\n",
    "# experiment with batch size (small batch size needs small learning rate)\n",
    "batch_size=32;\n",
    "n_epochs = 30;\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    #permute the training set for each epoch\n",
    "    #doing this on CPU. Alterntively, might be done on GPU, on the tensor version of x_train, y_train\n",
    "    permutation = np.random.permutation(x_train.shape[0]);\n",
    "    x_train=x_train[permutation,:]\n",
    "    y_train=y_train[permutation,:]\n",
    "\n",
    "    accuracy = 0.0;\n",
    "\n",
    "    #go through the training set in batches of size batch_size\n",
    "    for j in range(0,n_train,batch_size):\n",
    "        t_x_train = torch.tensor(x_train[j:j+batch_size,:],requires_grad=False,device=device);\n",
    "        t_y_train = torch.tensor(y_train[j:j+batch_size,:],requires_grad=False,device=device);\n",
    "\n",
    "        # clear previous gradient calculations\n",
    "        optimizer.zero_grad();\n",
    "        \n",
    "        # calculate model predictions\n",
    "        first_layer = torch.matmul(t_x_train,W1)+b1\n",
    "        activations_first_layer = 2.0 / (1.0 + torch.exp(-first_layer)) -1.0;\n",
    "\n",
    "        # done with first layer, it will serve as input to the second layer\n",
    "        second_layer = torch.matmul(activations_first_layer,W2)+b2\n",
    "        activations_second_layer = 1.0 / (1.0 + torch.exp(-second_layer));\n",
    "        # done with second layer\n",
    "\n",
    "        # but we need to normalize it (get softmax) for cross-entropy loss/risk\n",
    "        sum_activations = torch.sum(activations_second_layer,dim=1,keepdim=True)\n",
    "        normalized_activations = torch.div(activations_second_layer, sum_activations);\n",
    "        risk = -1.0 * torch.mean(torch.sum(torch.multiply(t_y_train,torch.log(normalized_activations)),dim=1 ) );\n",
    "\n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "        \n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        \n",
    "\n",
    "        batch_risk=risk.item();\n",
    "\n",
    "        true_class = np.argmax(t_y_train.detach().cpu().numpy(),axis=1)\n",
    "        pred_class = np.argmax(normalized_activations.detach().cpu().numpy(),axis=1)\n",
    "        accuracy += np.count_nonzero(true_class == pred_class);\n",
    "        \n",
    "    # after all the batches in this epoch are done, we calculate test set risk and accuracy\n",
    "    # we don't need gradients, so we turn them off\n",
    "        \n",
    "    with (torch.no_grad()):\n",
    "        first_layer = torch.matmul(t_x_test,W1)+b1\n",
    "        activations_first_layer = 2.0 / (1.0 + torch.exp(-first_layer)) -1.0;\n",
    "        second_layer = torch.matmul(activations_first_layer,W2)+b2\n",
    "        activations_second_layer = 1.0 / (1.0 + torch.exp(-second_layer));\n",
    "        sum_activations = torch.sum(activations_second_layer,dim=1,keepdim=True)\n",
    "        test_normalized_activations = torch.div(activations_second_layer, sum_activations);\n",
    "        \n",
    "        #calculate loss\n",
    "\n",
    "        test_risk = -1.0 * torch.mean(torch.sum(torch.multiply(t_y_test,torch.log(test_normalized_activations)),dim=1 ) );\n",
    "\n",
    "        test_true_class = np.argmax(t_y_test.detach().cpu().numpy(),axis=1)\n",
    "        test_pred_class = np.argmax(test_normalized_activations.detach().cpu().numpy(),axis=1)\n",
    "        test_accuracy = np.count_nonzero(test_true_class == test_pred_class)/test_pred_class.shape[0];\n",
    "        test_error = 1.0 - test_accuracy;\n",
    "        \n",
    "    accuracy = accuracy / float(x_train.shape[0])\n",
    "    print(i,batch_risk,test_risk.item(),accuracy,test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLBkA1bMezfX"
   },
   "source": [
    "This is the same code as above. We just created a pytorch Module to encapsulate the linear layer - that is, hold the W & b, and operations on it, inside a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz50v4RsKhjr"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#read in the dataset, convert to numpy\n",
    "\n",
    "n_classes = 10;\n",
    "n_features = 28*28;\n",
    "\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "x_train = full_train_dataset.data.numpy().reshape(-1,n_features).astype(dtype=np.float32)/255.0;\n",
    "x_test = full_test_dataset.data.numpy().reshape(-1,n_features).astype(dtype=np.float32)/255.0;\n",
    "\n",
    "y_train_cat = full_train_dataset.targets.numpy()\n",
    "y_test_cat = full_test_dataset.targets.numpy()\n",
    "\n",
    "dd_y=pd.DataFrame(data=y_train_cat)\n",
    "y_train=pd.get_dummies(dd_y,columns = list(dd_y.columns)).to_numpy().astype(dtype=np.float32);\n",
    "\n",
    "dd_y=pd.DataFrame(data=y_test_cat)\n",
    "y_test=pd.get_dummies(dd_y,columns = list(dd_y.columns)).to_numpy().astype(dtype=np.float32);\n",
    "\n",
    "\n",
    "# permute the dataset\n",
    "permutation = np.random.permutation(x_train.shape[0]);\n",
    "x_train=x_train[permutation,:]\n",
    "y_train=y_train[permutation,:]\n",
    "\n",
    "# select smaller subset (out of 50000)\n",
    "subset_size = 50000;\n",
    "x_train=x_train[:subset_size,:]\n",
    "y_train=y_train[:subset_size,:]\n",
    "\n",
    "\n",
    "n_train=x_train.shape[0];\n",
    "\n",
    "# create tensor variables for data, we do not need gradient w.r.t. to them\n",
    "t_x_test=torch.tensor(x_test,requires_grad=False,device=device);\n",
    "t_y_test=torch.tensor(y_test,requires_grad=False,device=device);\n",
    "\n",
    "# create a layer - we inherit from torch.nn.Module\n",
    "class MyLinear(torch.nn.Module):\n",
    "    def __init__(self,dim_in,dim_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate four parameters and assign them as\n",
    "        member parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_in=dim_in;\n",
    "        self.dim_out=dim_out;\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(dim_in,dim_out))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(dim_out))\n",
    "        torch.nn.init.kaiming_uniform_(self.weight);\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        return torch.addmm(self.bias, x, self.weight)\n",
    "\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        \"\"\"\n",
    "        This function will allow the layer to \"present itself\" if we want to print it\n",
    "        \"\"\"\n",
    "        return 'out={}, in={}'.format(\n",
    "            self.dim_out, self.dim_in\n",
    "        )\n",
    "\n",
    "\n",
    "# number of activations (neurons) in the hidden layer\n",
    "n_hidden = 100;\n",
    "\n",
    "\n",
    "lin1=MyLinear(dim_in=n_features,dim_out=n_hidden).to(device)\n",
    "lin2=MyLinear(dim_in=n_hidden,dim_out=n_classes).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.0001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.Adam(list(lin1.parameters())+list(lin2.parameters()),lr=learning_rate)\n",
    "\n",
    "# experiment with batch size (small batch size needs small learning rate)\n",
    "batch_size=32;\n",
    "n_epochs = 30;\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    #permute the training set for each epoch\n",
    "    #doing this on CPU. Alterntively, might be done on GPU, on the tensor version of x_train, y_train\n",
    "    permutation = np.random.permutation(x_train.shape[0]);\n",
    "    x_train=x_train[permutation,:]\n",
    "    y_train=y_train[permutation,:]\n",
    "\n",
    "    accuracy = 0.0;\n",
    "\n",
    "    #go through the training set in batches of size batch_size\n",
    "    for j in range(0,n_train,batch_size):\n",
    "        t_x_train = torch.tensor(x_train[j:j+batch_size,:],requires_grad=False,device=device);\n",
    "        t_y_train = torch.tensor(y_train[j:j+batch_size,:],requires_grad=False,device=device);\n",
    "\n",
    "        # clear previous gradient calculations\n",
    "        optimizer.zero_grad();\n",
    "        \n",
    "        # calculate model predictions\n",
    "        first_layer = lin1(t_x_train)\n",
    "        activations_first_layer = 2.0 / (1.0 + torch.exp(-first_layer)) -1.0;\n",
    "\n",
    "        # done with first layer, it will serve as input to the second layer\n",
    "        second_layer = lin2(activations_first_layer)\n",
    "        activations_second_layer = 1.0 / (1.0 + torch.exp(-second_layer));\n",
    "        # done with second layer\n",
    "\n",
    "        # but we need to normalize it (get softmax) for cross-entropy loss/risk\n",
    "        sum_activations = torch.sum(activations_second_layer,dim=1,keepdim=True)\n",
    "        normalized_activations = torch.div(activations_second_layer, sum_activations);\n",
    "        risk = -1.0 * torch.mean(torch.sum(torch.multiply(t_y_train,torch.log(normalized_activations)),dim=1 ) );\n",
    "\n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "        \n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        \n",
    "\n",
    "        batch_risk=risk.item();\n",
    "\n",
    "        true_class = np.argmax(t_y_train.detach().cpu().numpy(),axis=1)\n",
    "        pred_class = np.argmax(normalized_activations.detach().cpu().numpy(),axis=1)\n",
    "        accuracy += np.count_nonzero(true_class == pred_class);\n",
    "        \n",
    "    # after all the batches in this epoch are done, we calculate test set risk and accuracy\n",
    "    # we don't need gradients, so we turn them off\n",
    "        \n",
    "    with (torch.no_grad()):\n",
    "        first_layer = lin1(t_x_test)\n",
    "        activations_first_layer = 2.0 / (1.0 + torch.exp(-first_layer)) -1.0;\n",
    "        second_layer = lin2(activations_first_layer)\n",
    "        activations_second_layer = 1.0 / (1.0 + torch.exp(-second_layer));\n",
    "        sum_activations = torch.sum(activations_second_layer,dim=1,keepdim=True)\n",
    "        test_normalized_activations = torch.div(activations_second_layer, sum_activations);\n",
    "        \n",
    "        #calculate loss\n",
    "\n",
    "        test_risk = -1.0 * torch.mean(torch.sum(torch.multiply(t_y_test,torch.log(test_normalized_activations)),dim=1 ) );\n",
    "\n",
    "        test_true_class = np.argmax(t_y_test.detach().cpu().numpy(),axis=1)\n",
    "        test_pred_class = np.argmax(test_normalized_activations.detach().cpu().numpy(),axis=1)\n",
    "        test_accuracy = np.count_nonzero(test_true_class == test_pred_class)/test_pred_class.shape[0];\n",
    "        test_error = 1.0 - test_accuracy;\n",
    "        \n",
    "    accuracy = accuracy / float(x_train.shape[0])\n",
    "    print(i,batch_risk,test_risk.item(),accuracy,test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhYwMJQMfF7b"
   },
   "source": [
    "It's cleaner to create the whole network as a single Module, and use already existing pytorch Modules for the individual layers.\n",
    "\n",
    "We will also use dataloader that simplifies creating random batches of data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgNWusapPJYX"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#read in the dataset, convert to numpy\n",
    "\n",
    "n_classes = 10;\n",
    "n_features = 28*28;\n",
    "\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "batch_size=32;\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(full_test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "# create the whole neural network as one module(inherit from nn.Module)\n",
    "# inside, reuse existing PyTorch modules, like nn.Linear\n",
    "class MyNet(torch.nn.Module):\n",
    "    # architecture of the network is specified in the constructor\n",
    "    def __init__(self,dim_in,dim_out,dim_hidden):\n",
    "        super().__init__()\n",
    "        self.dim_in=dim_in;\n",
    "        self.dim_out=dim_out;\n",
    "        self.dim_hidden=dim_hidden;\n",
    "        self.lin1 = torch.nn.Linear(dim_in,dim_hidden);\n",
    "        self.a1 = torch.nn.functional.tanh;\n",
    "        self.lin2 = torch.nn.Linear(dim_hidden,dim_out)\n",
    "        self.a2 = torch.nn.functional.log_softmax;\n",
    "    \n",
    "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x);\n",
    "        x = self.a1(x);\n",
    "        x = self.lin2(x);\n",
    "        x = self.a2(x);\n",
    "        return x;\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'out={}, in={}'.format(\n",
    "            self.dim_out, self.dim_in\n",
    "        )\n",
    "\n",
    "\n",
    "# number of activations (neurons) in the hidden layer\n",
    "n_hidden = 100;\n",
    "\n",
    "\n",
    "model=MyNet(dim_in=n_features,dim_hidden=n_hidden,dim_out=n_classes).to(device)\n",
    "\n",
    "criterion = torch.nn.NLLLoss().to(device)\n",
    "\n",
    "\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.0001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# experiment with batch size (small batch size needs small learning rate)\n",
    "batch_size=32;\n",
    "n_epochs = 30;\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    for j, data in enumerate(trainloader):\n",
    "      \n",
    "        inputs, labels = data        \n",
    "        inputs=inputs.to(device);\n",
    "        labels=labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "\n",
    "        outputs = model(inputs.view(-1,28*28));\n",
    "        risk = criterion(outputs, labels);\n",
    "  \n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "        \n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        \n",
    "\n",
    "        batch_risk=risk.item();\n",
    "    with (torch.no_grad()):\n",
    "      correct = 0;\n",
    "      for j, data in enumerate(testloader):\n",
    "        \n",
    "          inputs, labels = data        \n",
    "          inputs=inputs.to(device);\n",
    "          labels=labels.to(device);\n",
    "          outputs = model(inputs.view(-1,28*28));\n",
    "          pred = outputs.data.max(1, keepdim=True)[1]\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
    "    print(i, batch_risk, correct / len(testloader.dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q539Uf3kfTQT"
   },
   "source": [
    "In most cases, we don't even need to create a class, we can just use nn.Sequential to contain the list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQPVPlSyTLeK"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#read in the dataset, convert to numpy\n",
    "\n",
    "n_classes = 10;\n",
    "n_features = 28*28;\n",
    "\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor() )\n",
    "full_test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor() )\n",
    "\n",
    "batch_size=64;\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(full_test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# number of activations (neurons) in the hidden layer\n",
    "n_hidden = 100;\n",
    "\"\"\"\n",
    "In simple cases (like: layer after layer forming a sequence)\n",
    "we can use a PyTorch container module\n",
    "instead of manually defining a new module to define our network\n",
    "\"\"\"\n",
    "#\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_features,n_hidden),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(n_hidden,n_classes),\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "criterion = torch.nn.NLLLoss().to(device)\n",
    "\n",
    "\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# experiment with batch size (small batch size needs small learning rate)\n",
    "n_epochs = 30;\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    for j, data in enumerate(trainloader):\n",
    "      \n",
    "        inputs, labels = data        \n",
    "        inputs=inputs.to(device);\n",
    "        labels=labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "\n",
    "        outputs = model(inputs.view(-1,28*28));\n",
    "        risk = criterion(outputs, labels);\n",
    "  \n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "        \n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        \n",
    "\n",
    "        batch_risk=risk.item();\n",
    "    with (torch.no_grad()):\n",
    "      correct = 0;\n",
    "      for j, data in enumerate(testloader):\n",
    "        \n",
    "          inputs, labels = data        \n",
    "          inputs=inputs.to(device);\n",
    "          labels=labels.to(device);\n",
    "          outputs = model(inputs.view(-1,28*28));\n",
    "          pred = outputs.data.max(1, keepdim=True)[1]\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
    "    print(i, batch_risk, correct / len(testloader.dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2ng0H6-x9eR"
   },
   "source": [
    "# Convolutional Networks for Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEFGfMeyfq-b"
   },
   "source": [
    "Insead of just linear layers that don't consider spatial arrangement of pixels, we should use convolutional layers and pooling layers. We added BatchNorm and Dropout.\n",
    "\n",
    "We also added a learning rate Scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8geyvVZUxp3"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn;\n",
    "import torch.nn.functional as F;\n",
    "\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#read in the dataset, convert to numpy\n",
    "\n",
    "\n",
    "full_train_dataset = datasets.MNIST(root='./data', train=True, download=True, \n",
    "                             transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]) )\n",
    "full_test_dataset = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]) )\n",
    "\n",
    "batch_size=64;\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(full_test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "class ConvNetWithBatchNorm(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(ConvNetWithBatchNorm, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5),         # (N, 1, 28, 28) -> (N,  3, 24, 24)\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  # (N, 3, 24, 24) -> (N,  3, 12, 12)\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=6)           # (N, 3, 12, 12) -> (N,  6, 10, 10) \n",
    "        )\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)   # (N, 6, 10, 10) -> (N,  6, 5, 5)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(150, 50),         # (N, 150) -> (N, 50)\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,10)            # (N, 50) -> (N, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.features1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model=ConvNetWithBatchNorm().to(device);\n",
    "criterion = F.nll_loss;\n",
    "\n",
    "\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "n_epochs = 30;\n",
    "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
    "print(num_updates)\n",
    "warmup_steps=1000;\n",
    "def warmup_linear(x):\n",
    "    if x < warmup_steps:\n",
    "        lr=x/warmup_steps\n",
    "    else:\n",
    "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
    "    return lr;\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
    "\n",
    "# experiment with batch size (small batch size needs small learning rate)\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    for j, data in enumerate(trainloader):\n",
    "      \n",
    "        inputs, labels = data        \n",
    "        inputs=inputs.to(device);\n",
    "        labels=labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "\n",
    "        outputs = model(inputs);\n",
    "        risk = criterion(outputs, labels);\n",
    "  \n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "        \n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        scheduler.step();\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        batch_risk=risk.item();\n",
    "    with (torch.no_grad()):\n",
    "      correct = 0;\n",
    "      for j, data in enumerate(testloader):\n",
    "        \n",
    "          inputs, labels = data        \n",
    "          inputs=inputs.to(device);\n",
    "          labels=labels.to(device);\n",
    "          outputs = model(inputs);\n",
    "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
    "    print(i, batch_risk, correct / len(testloader.dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhjyBqb8vpbJ"
   },
   "source": [
    "Almost the same network, but on a more complicated CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeeFJnKPmvfX"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# train using GPU, if not available on your machine, use google colab.\n",
    "\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn;\n",
    "import torch.nn.functional as F;\n",
    "\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#read in the dataset\n",
    "\n",
    "\n",
    "num_classes=10;\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
    "                             transform=transform )\n",
    "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                             transform=transform )\n",
    "\n",
    "batch_size=64;\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(full_test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "# create a neural network (inherit from nn.Module)\n",
    "class ConvNetWithBatchNorm(nn.Module):\n",
    "    # architecture of the network is specified in the constructor\n",
    "    def __init__(self): \n",
    "        super(ConvNetWithBatchNorm, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),         \n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),  \n",
    "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=3),\n",
    "            nn.BatchNorm2d(num_features=12)           \n",
    "        )\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)   \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(12*6*6, 50),         \n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50,num_classes)            \n",
    "        )\n",
    "        \n",
    "    # here we specify the computation (forward phase of training) how \"x\" is transfered into output \"y\"\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.features1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "    # constructor and forward() - that is all we need, the rest is implemented in the nn.Module and we inherit it\n",
    "\n",
    "# create an instance of the network\n",
    "model=ConvNetWithBatchNorm().to(device);\n",
    "criterion = F.nll_loss;\n",
    "\n",
    "\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# we add a learning rate scheduler, which will modify the learning rate during training\n",
    "# will initially start low, then increase it (\"warm up\"), and then gradually descrease it\n",
    "n_epochs = 30;\n",
    "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
    "print(num_updates)\n",
    "warmup_steps=1000;\n",
    "def warmup_linear(x):\n",
    "    if x < warmup_steps:\n",
    "        lr=x/warmup_steps\n",
    "    else:\n",
    "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
    "    return lr;\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    for j, data in enumerate(trainloader):\n",
    "      \n",
    "        inputs, labels = data        \n",
    "        inputs=inputs.to(device);\n",
    "        labels=labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "\n",
    "        #forward phase - predictions by the model\n",
    "        outputs = model(inputs);\n",
    "        #forward phase - risk/loss for the predictions\n",
    "        risk = criterion(outputs, labels);\n",
    "  \n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "        \n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        scheduler.step();\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        batch_risk=risk.item();\n",
    "    with (torch.no_grad()):\n",
    "      correct = 0;\n",
    "      for j, data in enumerate(testloader):\n",
    "        \n",
    "          inputs, labels = data        \n",
    "          inputs=inputs.to(device);\n",
    "          labels=labels.to(device);\n",
    "          outputs = model(inputs);\n",
    "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
    "    print(i, batch_risk, correct / len(testloader.dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hMTDEWpvvvy"
   },
   "source": [
    "Instead of training our own network, we can reuse pre-trained ResNet18, and just fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgepfpoIj91_"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd;\n",
    "from scipy.stats import zscore\n",
    "import torch as torch;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch.nn as nn;\n",
    "import torch.nn.functional as F;\n",
    "\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#read in the dataset, convert to numpy\n",
    "\n",
    "num_classes=10;\n",
    "\n",
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
    "                             transform=transform )\n",
    "full_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                             transform=transform )\n",
    "\n",
    "batch_size=64;\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(full_test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "criterion = F.nll_loss;\n",
    "\n",
    "\n",
    "# this optimizer will do gradient descent for us\n",
    "# experiment with learning rate and optimizer type\n",
    "learning_rate = 0.0001;\n",
    "# note that we have to add all weights&biases, for both layers, to the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "n_epochs = 3;\n",
    "num_updates = n_epochs*int(np.ceil(len(trainloader.dataset)/batch_size))\n",
    "print(num_updates)\n",
    "warmup_steps=200;\n",
    "def warmup_linear(x):\n",
    "    if x < warmup_steps:\n",
    "        lr=x/warmup_steps\n",
    "    else:\n",
    "        lr=max( (num_updates - x ) / (num_updates - warmup_steps), 0.)\n",
    "    return lr;\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, warmup_linear);\n",
    "\n",
    "# experiment with batch size (small batch size needs small learning rate)\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    for j, data in enumerate(trainloader):\n",
    "      \n",
    "        inputs, labels = data        \n",
    "        inputs=inputs.to(device);\n",
    "        labels=labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "\n",
    "        outputs = F.log_softmax(model(inputs),dim=1);\n",
    "        risk = criterion(outputs, labels);\n",
    "  \n",
    "        # calculate gradients\n",
    "        risk.backward();\n",
    "        \n",
    "        # take the gradient step\n",
    "        optimizer.step();\n",
    "        scheduler.step();\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        batch_risk=risk.item();\n",
    "    with (torch.no_grad()):\n",
    "      correct = 0;\n",
    "      for j, data in enumerate(testloader):\n",
    "        \n",
    "          inputs, labels = data        \n",
    "          inputs=inputs.to(device);\n",
    "          labels=labels.to(device);\n",
    "          outputs = F.log_softmax(model(inputs),dim=1);\n",
    "          pred = outputs.data.max(dim=1, keepdim=True)[1]\n",
    "          correct += pred.eq(labels.data.view_as(pred)).sum().item();\n",
    "    print(i, batch_risk, correct / len(testloader.dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTD1ggEExnBg"
   },
   "source": [
    "# Transformer Models for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2_20RY7zDwM"
   },
   "source": [
    "Setup and installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRG5QhWWEDKk"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/huggingface/transformers/v4.5.1-release/examples/text-classification/run_glue.py -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUP7zxN2NKUi"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets -qq\n",
    "!pip install wandb -qq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2Uxw0ypHvVn"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRhpT306yi4n"
   },
   "source": [
    "Let's try our first Transformer model for text. We use run_glue.py script to fine-tune towards MRPC task (paraphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hklumA6JyNAn"
   },
   "outputs": [],
   "source": [
    "%env TASK_NAME=MRPC\n",
    "\n",
    "!python run_glue.py \\\n",
    "  --model_name_or_path bert-base-cased \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 0 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --output_dir my_mrpc_init \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gINQNNmiyx7N"
   },
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwu9Xr6HyRdM"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_mrpc_init\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"my_mrpc_init\")\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "sequence_0 = \"Tall mountains frequently have snow on top\"\n",
    "sequence_1 = \"It's rare to have a beach at the top of the mountain\"\n",
    "sequence_2 = \"Snow is often at the top of high mountains\"\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVN64b5Ey04h"
   },
   "source": [
    "Let train for a bit longer (3 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7iu_-URHiEC"
   },
   "source": [
    "Go to Runtime->Change Runtime Type and select GPU. Try also running it on CPU, and observe the difference in running time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MjJToBEE_Nv"
   },
   "outputs": [],
   "source": [
    "%env TASK_NAME=MRPC\n",
    "\n",
    "!python run_glue.py \\\n",
    "  --model_name_or_path bert-base-cased \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --output_dir my_mrpc \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1197uu_QJ1nM"
   },
   "source": [
    "Now we can load the trained model again and see how well it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjZWobgtJEkx"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"my_mrpc\")\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "sequence_0 = \"Tall mountains frequently have snow on top\"\n",
    "sequence_1 = \"It's rare to have a beach at the top of the mountain\"\n",
    "sequence_2 = \"Snow is often at the top of high mountains\"\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrlE6ox2J7eI"
   },
   "source": [
    "For standard tasks, we don't have to train outselves, there are pre-trained, fine-tuned models ready for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dbUXVjmHQrg"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
    "sequence_0 = \"Tall mountains frequently have snow on top\"\n",
    "sequence_1 = \"It's rare to have a beach at the top of the mountain\"\n",
    "sequence_2 = \"Snow is often at the top of high mountains\"\n",
    "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"pt\")\n",
    "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "paraphrase_classification_logits = model(**paraphrase).logits\n",
    "not_paraphrase_classification_logits = model(**not_paraphrase).logits\n",
    "paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "# Should be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
    "# Should not be paraphrase\n",
    "for i in range(len(classes)):\n",
    "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmZ9mx1bIxJG"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTj9DtNY_giD"
   },
   "source": [
    "Let's try a different task: sentiment analysis. We'll use a pre-trained, fine-tuned model again. Observe the size of the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XofHCh3UqPy"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "result = nlp(\"The huggingface library is quite comprehensive and simple to use\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "result = nlp(\"Coding it all in pytorch would be a dounting task\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3loQLDG_obY"
   },
   "source": [
    "Transformers can also be used to generate text. For example, to create summaries. Again, observe the size of the downloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QoBgvIBoWYqV"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import textwrap\n",
    "\n",
    "to_tokenize = \"The idea that aliens had frequented our planet had been circulating among ufologists since the postwar years, when a Polish emigre, George Adamski, claimed to have rendezvoused with a race of kindly, Nordic-looking Venusians who were disturbed by the domestic and interplanetary effects of nuclear-bomb tests. In the summer of 1947, an alien spaceship was said to have crashed near Roswell, New Mexico. Conspiracy theorists believed that vaguely anthropomorphic bodies had been recovered there, and that the crash debris had been entrusted to private military contractors, who raced to unlock alien hardware before the Russians could. (Documents unearthed after the fall of the Soviet Union suggested that the anxiety about an arms race supercharged by alien technology was mutual.) All of this, ufologists claimed, had been covered up by Majestic 12, a clandestine, para-governmental organization convened under executive order by President Truman. President Kennedy was assassinated because he planned to level with Premier Khrushchev; Kennedy had confided in Marilyn Monroe, thereby sealing her fate. Representative Steven Schiff, of New Mexico, spent years trying to get to the bottom of the Roswell incident, only to die of 'cancer'.\"\n",
    "\n",
    "# Initialize the HuggingFace summarization pipeline\n",
    "summarizer = pipeline(\"summarization\",model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "summarized = summarizer(to_tokenize, min_length=40, max_length=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABRCQoAbYOmb"
   },
   "outputs": [],
   "source": [
    "# Print original text\n",
    "print(textwrap.fill(to_tokenize, 80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOlWco4UYP6c"
   },
   "outputs": [],
   "source": [
    "# Print summarized text\n",
    "summary=summarized[0]['summary_text']\n",
    "print(textwrap.fill(summary, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOSka5EkA9jt"
   },
   "source": [
    "Let's look at the code that actually uses the model (T5) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4ArJBbgYK9O"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "to_tokenize = \"The idea that aliens had frequented our planet had been circulating among ufologists since the postwar years, when a Polish emigre, George Adamski, claimed to have rendezvoused with a race of kindly, Nordic-looking Venusians who were disturbed by the domestic and interplanetary effects of nuclear-bomb tests. In the summer of 1947, an alien spaceship was said to have crashed near Roswell, New Mexico. Conspiracy theorists believed that vaguely anthropomorphic bodies had been recovered there, and that the crash debris had been entrusted to private military contractors, who raced to unlock alien hardware before the Russians could. (Documents unearthed after the fall of the Soviet Union suggested that the anxiety about an arms race supercharged by alien technology was mutual.) All of this, ufologists claimed, had been covered up by Majestic 12, a clandestine, para-governmental organization convened under executive order by President Truman. President Kennedy was assassinated because he planned to level with Premier Khrushchev; Kennedy had confided in Marilyn Monroe, thereby sealing her fate. Representative Steven Schiff, of New Mexico, spent years trying to get to the bottom of the Roswell incident, only to die of 'cancer'.\"\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "inputs = tokenizer.encode(\"summarize: \" + to_tokenize, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8FumnE1BnbG"
   },
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6dVafmwYe0b",
    "outputId": "fb0f2b77-96b4-423d-ccf8-096d4d49297e"
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItM_zSCkccVH"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsGESYexD5Ru"
   },
   "source": [
    "Let finally try translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RC7DuTQQD4fc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "to_tokenize = \"The idea that aliens had frequented our planet had been circulating among ufologists since the postwar years, when a Polish emigre, George Adamski, claimed to have rendezvoused with a race of kindly, Nordic-looking Venusians who were disturbed by the domestic and interplanetary effects of nuclear-bomb tests. In the summer of 1947, an alien spaceship was said to have crashed near Roswell, New Mexico. Conspiracy theorists believed that vaguely anthropomorphic bodies had been recovered there, and that the crash debris had been entrusted to private military contractors, who raced to unlock alien hardware before the Russians could. (Documents unearthed after the fall of the Soviet Union suggested that the anxiety about an arms race supercharged by alien technology was mutual.) All of this, ufologists claimed, had been covered up by Majestic 12, a clandestine, para-governmental organization convened under executive order by President Truman. President Kennedy was assassinated because he planned to level with Premier Khrushchev; Kennedy had confided in Marilyn Monroe, thereby sealing her fate. Representative Steven Schiff, of New Mexico, spent years trying to get to the bottom of the Roswell incident, only to die of 'cancer'.\"\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "# T5 uses a max_length of 512 so we cut the article to 512 tokens.\n",
    "inputs = tokenizer.encode(\"translate English to German: \" + to_tokenize, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "outputs = model.generate(inputs, max_length=512, min_length=40, num_beams=4, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQw_uJ6BuBO0"
   },
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhcovSW0u04w"
   },
   "source": [
    "We are using V1 compatibility, to illustrate static computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00tG8oHluEI_"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import tensorflow as tf;\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "minimum=[-.25,2]\n",
    "\n",
    "# THIS SCRIPT PERFORMS PROJECTED GRADIENT DESCENT ON FUNCTION F, \n",
    "# ASSUMING Q(the feasible region) is w1>=0, w2>=0\n",
    "\n",
    "\n",
    "def f(w):\n",
    "    shiftedW=w-np.array(minimum);\n",
    "    return tf.reduce_sum(tf.multiply(shiftedW,shiftedW));\n",
    "\n",
    "#define starting value of W for gradient descent\n",
    "#here, W is a 2D vector\n",
    "initialW=np.random.randn(2)\n",
    "\n",
    "#create a shared variable (i.e. a variable that persists between calls to a tensorflow function)\n",
    "w = tf.Variable(initialW,name=\"w\");\n",
    "\n",
    "#define output of applying f to w\n",
    "#out goal will be to minimize f(w), i.e. find w with lowest possible f(w)\n",
    "z=f(w);\n",
    "\n",
    "# if you want more accurate result, replace step size 0.01 with something smaller\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(z)\n",
    "\n",
    "\n",
    "#initialize tensorflow session\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "with sess:\n",
    "    # hard-coded number of steps, could be too little, may need to be increased\n",
    "    for i in range(300):\n",
    "      #perform gradient step\n",
    "      train.run();\n",
    "      #get the numpy vector with current value of w\n",
    "      w_value=w.eval();\n",
    "      # run proximal operator (here it's simple, just replace negative values with 0)\n",
    "      new_w_value=np.maximum(w_value,0);\n",
    "      print((w_value,new_w_value))\n",
    "      # update tensorflow value using numpy value\n",
    "      new_w_assign = tf.compat.v1.assign(w,new_w_value);\n",
    "      sess.run(new_w_assign);\n",
    "\n",
    "#sess.close()\n",
    "\n",
    "print(\"True minimum: \"+str(np.maximum(minimum,0)));\n",
    "print(\"Found minimum:\"+str(new_w_value));\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deep_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
